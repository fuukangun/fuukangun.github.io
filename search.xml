<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Java注解(Annotation)入门]]></title>
      <url>%2F2017%2F04%2F23%2FJava%E6%B3%A8%E8%A7%A3(Annotation)%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[概念什么是注解(Annotation)？用一个词来进行描述，那就是元数据，即一种描述数据的数据。所以，可以说注解就是源代码的元数据。Java1.5引入了注解，而在此之前，XML被广泛地应用于描述数据，甚至在注解出现后依旧如此。开发人员对于使用注解还是XML经常会产生争论，因为各自的优缺点，所以还没完全用注解来替代XML的地步。 注解的类型元注解@Target@Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。 作用：用于描述注解的使用范围（即：被描述的注解可以用在什么地方） 取值(ElementType)： CONSTRUCTOR:用于描述构造器 FIELD:用于描述域 LOCAL_VARIABLE:用于描述局部变量 METHOD:用于描述方法 PACKAGE:用于描述包 PARAMETER:用于描述参数 TYPE:用于描述类、接口(包括注解类型) 或enum声明 @Retention@Retention定义了该Annotation被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对 Annotation的“生命周期”限制。 作用：表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效） 取值(RetentionPoicy)： SOURCE:在源文件中有效（即源文件保留） CLASS:在class文件中有效（即class保留） RUNTIME:在运行时有效（即运行时保留） @Documented@Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。 @Inherited@Inherited 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。 ⚠️注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。 当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。 Java内建注解@Override当我们想要复写超类中的方法时，我们需要使用该注解去告知编译器我们想要复写这个方法。这样一来当父类中的方法移除或者发生更改时编译器将提示错误信息。 @Deprecated当我们希望编译器知道某一个方法不建议使用时(或许我们会在下一个版本中去掉该方法)，我们应该使用这个注解。Java在JavaDoc中推荐使用该注解，我们应该告知该方法不推荐使用，并且提供替代的方法。 @SuppressWarnings这个注解仅仅是告诉编译器忽略特定的警告信息，例如在泛型中使用原生数据类型。它的保留策略是SOURCE并且被编译器丢弃。 Java自定义注解创建自定义注解和创建一个接口相似，但是注解的interface关键字需要以@符号开头。我们可以为注解声明方法。 注意点： 注解方法不能带有参数 注解方法返回值类型限定为：基本类型、String、Enums、Annotation或者是这些类型的数组 注解方法可以有默认值 注解本身能够包含元注解，元注解被用来注解其它注解 一个小实例：下面这个注解，用于读取Excel的行数据，并且转化为Student对象。通过Java反射机制，可以把每一行中，第一列的数据赋值给Student对象的name，第二列赋值给sex，第三列赋值给age，第四列赋值给address。具体的解析代码不在此处赘述。 123456789101112131415package com.demo.annotation;import java.lang.annotation.*;@Documented//文档@Retention(RetentionPolicy.RUNTIME)//在运行时可以获取@Target(&#123; ElementType.FIELD &#125;)//作用到类的域上面public @interface ImportColumnNo &#123; // 在excel第几列 public int value(); // set出错时的信息 public String errorMsg() default "";&#125; 1234567891011121314151617181920212223public calss Student &#123; @Getter @Setter @ImportColumnNo(value = 0) private String name; @Getter @Setter @ImportColumnNo(value = 1) private String sex; @Getter @Setter @ImportColumnNo(value = 2, errorMsg = "转化出错") private int age; @Getter @Setter @ImportColumnNo(value = 3) private String address; &#125; XML和注解(Annotation)的取舍XML建议使用的情况 外部jar包依赖bean配置 用注解无法实现，或者用注解无法轻易实现的情形 项目组内部达成一致的约定的地方 特殊的配置（如：定义一个map） 优：容易编辑，配置比较集中，方便修改，在大业务量的系统里面，通过XML配置会方便后人理解整个系统的架构，修改之后直接重启应用即可 缺：比较繁琐，类型不安全，配置形态丑陋,配置文件过多的时候难以管理 注解(Annotation)建议使用的情况除了上面4点，其他情况都可以用 优：方便，简洁，配置信息和 Java 代码放在一起，有助于增强程序的内聚性 缺：分散到各个class文件中，所以不宜维护，修改之后你需要重新打包，发布，重启应用 结束语因为需要做一个Excel模版里的行Row内容解析成对象的功能。不同的Excel模版对应不同的对象。为了减少每次对列Col的读取转换，所以学习了注解，通过注解来实现重复性的工作，使得代码更加优雅。当Excel模板变化的时候，只要在对象的属性上添加对应Exce第几列的注解即可，完全不用去修改业务逻辑代码。 正如本文简介所说，作为开发者的你，每当创建描述符性质的类或者接口时,一旦其中包含重复性的工作，就可以考虑使用注解(Annotation)来简化与自动化该过程。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java反射(Reflect)机制]]></title>
      <url>%2F2017%2F04%2F15%2FJava%E5%8F%8D%E5%B0%84(Reflect)%E6%9C%BA%E5%88%B6%2F</url>
      <content type="text"><![CDATA[什么是反射Java程序允许在运行期间，检测和修改自身信息，并且可以操作对象的属性和方法。这种动态获取类的信息以及动态调用对象的方法的功能来自于Java语言的反射（Reflection）机制。 Java反射框架主要提供以下功能： 在运行时判断任意一个对象所属的类； 在运行时构造任意一个类的对象； 在运行时判断任意一个类所具有的成员变量和方法； 在运行时调用任意一个对象的方法，包括private方法； ⚠️注意：是在程序运行时，而非编译时。 例如我们有一个类Person，一个类Student extend Person。 1Person person = new Student()； 那么对象person在编译时类型时Person，但是运行时的类型却是Student。 反射的用途反射最重要的用途是开发各种各样的通用框架。例如Spring等，程序需要根据配置文件或者注解，去动态的加载对象，这个时候就需要用到反射了。 使用反射反射的几个重要类 Field：描述类的域 Method：描述类的方法 Constructor：描述类的构造器 所有的域相关方法(包括获取超类的)： getFields getMethods getConstructors private和protected域(不包含超类)： gettDeclaredFields gettDeclaredMethods gettDeclaredConstructors ⚠️注意：如果想获取所有域，并且包含超类。只要循环调用Class. getSuperclass()方法，再调用上述获取的所有域的方法即可。当超类为Object时，可以跳出循环，不然会多获得两个成员变量。 加载类 加载类有三种方法，最常用的是Object.class。如果使用Class.forName函数，入参必须是完整的路径。 1234// 加载类的3种方法Class clazz = Class.forName("com.demo.reflect.Person");Class clazz1 = new Person().getClass();Class class2 = Person.class; 通过构造函数实例化类12345Class clazz = Person.class;Constructor c = clazz.getDeclaredConstructor(new Class[] &#123; String.class &#125;);// 由于构造函数是private的，所以需要屏蔽Java语言的访问检查c.setAccessible(true);Person p = (Person) c.newInstance(new Object[] &#123; "I'm a reflect name!" &#125;); 获取并调用类的无参方法12345Class clazz = Person.class;Constructor c = clazz.getConstructor(null);Person p = (Person) c.newInstance(null);Method method = clazz.getMethod("fun", null);method.invoke(p, null); 获取并调用类的含参方法12345Class clazz = Person.class;Constructor c = clazz.getConstructor(null);Person p = (Person) c.newInstance(null);Method method = clazz.getMethod("fun", new Class[] &#123; String.class &#125;);method.invoke(p, new Object[] &#123; "I'm a reflect method!" &#125;); 获取类的字段123456789101112Class clazz = Person.class;Constructor c = clazz.getDeclaredConstructor(new Class[] &#123; String.class &#125;);// 由于构造函数是 private 的，所以需要获取控制权限c.setAccessible(true);Person p = (Person) c.newInstance(new Object[] &#123; "I'm a reflect name!" &#125;);Field f = clazz.getField("name");Object value = f.get(p);Class type = f.getType();System.out.println(type);if (type.equals(String.class)) &#123; System.out.println((String) value);&#125; 使用反射的注意点 利用反射机制，开发者可以跨过权限的限制，一定程度上会影响封装性和安全。 反射会额外消耗一定的系统资源，如果大量使用，并不做任何缓存，会导致性能问题。可以参照对比apache和Spring的BeanCopy。 如果不需要动态的创建对象，那还是别用反射了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[转:Solr文本分析]]></title>
      <url>%2F2017%2F04%2F12%2F%E8%BD%AC-Solr%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[本文转自周金根的博客http://www.cnblogs.com/zhoujg/archive/2015/12/18/5054122.html Solr：文本分析文本分析是搜索引擎的核心工作之一，对文本包含许多处理步骤，比如：分词、大写转小写、词干化、同义词转化等。简单的说，文本分析就说将一个文本字段的值转为一个一个的token，然后被保存到Lucene的索引结构中被将来搜索用。当然，文本分析不仅在建立索引时有用，在查询时对对所输入的查询串也一样可以进行文本分析。在 Solr Schema设计 中我们介绍了许多Solr中的字段类型，其中最重要的是solr.TextField，这个类型可以进行分析器配置来进行文本分析。 接下来我们先来说说什么是分析器。 分析器一个分析器可以检查字段的文本信息，并且产生一个 token 流。分析器是 schema.xml 中元素的一个子元素，通常使用中，只有 solr.TextField 类型的字段会专门制定一个分析器。最简单配置一个分析器的方式是使用元素，制定这个元素的 class 属性为一个完整的 Java 类名。这些类名必须源自 org.apache.lucene.analysis.Analyzer 。 123&lt;fieldType name="nametext" class="solr.TextField"&gt; &lt;analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/&gt;&lt;/fieldType&gt; 在这个例子中，WhitespaceAnalyzer 这个类负责分析文本字段的内容并且产生出正确的 tokens。如果只是简单的文本，例如“this is a pig”，像这样的一个分析器的类足可以应付了，但是我们经常需要对字段内容做复杂的分析，这就需要把分析作为多个独立的简单步骤来进行处理了。以下是处理复杂分析的示例，在 元素（不是类属性）下添加分词器和过滤器的工厂类： 1234567&lt;fieldType name="nametext" class="solr.TextField"&gt; &lt;analyzer&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;filter class="solr.LowerCaseFilterFactory"/&gt; &lt;filter class="solr.StopFilterFactory"/&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 注意：需要说明的话solr.前缀的包，其实是指向 org.apache.solr.analysis 这个包 在这个例子中，在 元素没有指定分析器的类，而是一系列的类共同承担一个字段的分析器。文本首先传到列表的第一个元素（solr.StandardTokenizerFactory），然后再依次执行filter。简单的说就是经过Tokenizer分词之后，再继续处理，比如全转成小写、时态处理、去掉语气词等，产生出来的tokens 作为 terms 在字段的索引和查询时使用。现在我们来看下Solr示例Schema配置中的text_en_splitting字段类型的定义，看看它用了哪些分析组件。 12345678910111213141516171819202122232425262728293031323334353637383940 &lt;!-- A text field with defaults appropriate for English, plusaggressive word-splitting and autophrase features enabled.This field is just like text_en, except it addsWordDelimiterFilter to enable splitting and matching ofwords on case-change, alpha numeric boundaries, andnon-alphanumeric chars. This means certain compound wordcases will work, for example query "wi fi" will matchdocument "WiFi" or "wi-fi". --&gt; &lt;fieldType name="text_en_splitting" class="solr.TextField" positionIncrementGap="100" autoGeneratePhraseQueries="true"&gt; &lt;analyzer type="index"&gt; &lt;!--&lt;charFilter class="solr.MappingCharFilterFactory" mapping="mapping-ISOLatin1Accent.txt"/&gt;--&gt; &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt; &lt;!-- in this example, we will only use synonyms at query time &lt;filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/&gt; --&gt; &lt;!-- Case insensitive stop word removal. --&gt; &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt" /&gt; &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/&gt; &lt;filter class="solr.LowerCaseFilterFactory"/&gt; &lt;filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/&gt; &lt;filter class="solr.PorterStemFilterFactory"/&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt; &lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/&gt; &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt" /&gt; &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"/&gt; &lt;filter class="solr.LowerCaseFilterFactory"/&gt; &lt;filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/&gt; &lt;filter class="solr.PorterStemFilterFactory"/&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; Type属性可以指定为index或是query值，分别表示是索引时用的分析器，和查询时所用的分析器。如果在索引和查询时使用相同的分析器，你可以不指定type属性值。 分析器的配置中可以选用一个或多个字符过滤器（character filter），字符过滤器是对原始文本进行字符流级别的操作。它通常可以用于大小写转化，去除字母上标等等。在字符过滤器之后是分词器（Tokenizer），它是必须要配置的。分析器会使用分词器将字符流切分成词元（Token）系列，通常用在空格处切分这种简单的算法。后面的步骤是可选的，比如token过滤器（Token Filter）会对token进行许多种操作，最后产生的词元会被称为词（Term），即用于Lucene实际索引和查询的单位。 最后，我有必须对autoGeneratePhraseQueries布尔属性补充两句，这个属性只能用于文本域。如果在查询文本分析时产生了多个词元，比如Wi-Fi分词为Wi和Fi，那么默认情况下它们只是两个不同的搜索词，它们没有位置上的关系。但如果autoGeneratePhraseQueries被设置，那么这两个词元就构造了一个词组查询，即“WiFi”，所以索引中“WiFi”必须相邻才能被查询到。在新Solr版本中，默认它被设置为false。我不建议使用它。 在Admin上对字段进行分析在我们深入特定分析组件的细节之前，有必要去熟悉Solr的分析页面，它是一个很好的实验和查错工具，绝对不容错过。你将会用它来验证不同的分析配置，来找到你最想要的效果，你还可以用它来找到你认为应该会匹配的查询为什么没有匹配。在Solr的管理页面，你可以看到一个名为[Analysis]的链接，你进入后，会看到下面的界面。界面上的第一个选项是必选的，你可选择直接通过字段类型名称来选择类型，你也可以间接地通过一个字段的名字来选择自端类型。在上面的示例中，我选择了title字段 通过Schema Browser可以看到这个字段类型是 text_general点击灰色的 text_general，可以看到这个字段的分析器中定义的分词器和过滤器接下来，你可以分析索引或是查询文本，也可以两者同时分析。你需要输入些文本到文本框中以进行分析。将字段值放到Index文本框中，将查询文本放入Query文本框中，点击Analyze按钮看到一下文本处理结果，因为还没有中文处理，所以中文都被一个字一个字的分开处理了。 你可以选中verbose output来查看处理的详细信息，我希望你能自己试一下。 上图中每一行表示分析器处理链上的每一步的处理结果。比如第三个分析组件是LowerCaseFilter，它的处理结果就在第三行。前面的ST/SF/LCF应该是分词器和过滤器的简称。 下面我们接着来详细看看有哪些分词器和过滤器吧。 Character Filter字符过滤器在元素中定义，它是对字符流进行处理。字符过滤器种类不多。这个特性只有下面第一个介绍的比较常见。 MappingCharFilterFactory：它将一个字符（或字符串）映射到另一个，也可以映射为空。换言之，它是一个查找-替换的功能。在mapping属性中你可以指定一个配置文件。Solr的示例配置中包括了两个有用的映射配置文件： mapping-FoldToASCII.txt：一个丰富的将non-ASCII转化成ASCII的映射。如果想了解字符映射更多的细节，可以阅读这个文件顶部的注释。这个字符过滤器有一个类似的词元过滤器ASCIIFoldFilterFactory，这个词元过滤器运行速度更快，建议使用它。 maping-ISOLatinAccent.txt：一个更小的映射文件子集，它只能将ISO Latin1上标映射。FoldToASCII内容更丰富，所以不建议使用这个配置。 HTMLStripCharFilterFactory：它用于HTML和XML，它不要求它们格式完全正确。本质上它是移除所有的标记，只留下文本内容。移除脚本内容和格式元素。转义过的特殊字符被还原（比如&amp;）。 PatternReplaceCharFilterFactory：根据pattern属性中的正则表达式进行查找，并根据replacement属性中的值进行替换。它的实现需要一个缓冲区容器，默认设置为10000个字符，可以通过maxBlockChars进行配置。分词器和词元过滤器中也有正则表达式组件。所以你应该只在会影响分词的影响下使用它，比如对空格进行处理。 Tokenization分词器在元素中定义，它将一个字符流切分成词元序列，大部分它会去除不重要的符号，比如空字符和连接符号。 一个分析器有且只应有一个分词器，你可选的分词器如下： KeywordTokenizerFactory：这个分词器不进行任何分词！整个字符流变为单个词元。String域类型也有类似的效果，但是它不能配置文本分析的其它处理组件，比如大小写转换。任何用于排序和大部分Faceting功能的索引域，这个索引域只有能一个原始域值中的一个词元。 WhitespaceTokenizerFactory：文本由空字符切分（即，空格，Tab，换行）。 StandardTokenizerFactory：它是一个对大部分西欧语言通常的分词器。它从空白符和其它Unicode标准中的词分隔符处进行切分。空白符和分隔符会被移除。连字符也被认为是词的分隔符，这使得它不适合与WordDelimiterFilter一起用。 UAX29URLEmailTokenizer：它表现的与StandardTokenizer相似，但它多了一个识别e-mail，URL并将它们视为单个词元的特性。 ClassicTokenizerFactory：（曾经的StandardTokenizer）它是一个英语的通用分词器。对英语来说，它优于StandardTokenizer。它可以识别有点号的缩写词，比如I.B.M.。如果词元中包含数字它不会在连字符处分词，并可以将Email地址和主机名视为单个词元。并且ClassicFilter词元过滤器经常与这个分词器配合使用。ClassicFilter会移除缩写词中的点号，并将单引号（英语中的所有格）去除。它只能与ClassicTokenizer一起使用。 LetterTokenizerFactory：这个分词器将相邻的字母（由Unicode定义）都视为一个词元，并忽略其它字符。 LowerCaseTokenizerFactory：这个分词器功能上等同于LetterTokenizer加上LowerCaseFilter，但它运行更快。 PatternTokenizerFactory：这个基于正则表达式的分词器可以以下面两种方式工作： 通过一个指定模式切分文本，例如你要切分一个用分号分隔的列表，你可以写：。 只选择匹配的一个子集作为词元。比如：。组属性指定匹配的哪个组将被视为词元。如果你输入的文本是aaa ‘bbb’ ‘ccc’，那么词元就是bbb和ccc。 PathHierachyTokenizerFactory：这是一个可配置的分词器，它只处理以单个字符分隔的字符串，比如文件路径和域名。它在实现层次Faceting中很有用，或是可以过滤以某些路径下的文件。比如输入字符串是/usr/local/apache会被分词为三个词元：/usr，/usr/local，/usr/local/apache。这个分词器有下面四个选项： Delimiter：分隔字符：默认为/ Replace：将分隔字符替换为另一字符（可选） Reverse：布尔值表明是否层次是从右边开始，比如主机名，默认：false。 Skip：忽略开头的多少个词元，默认为0。 WikipediaTokenizerFactory：一个用于Mediawiki语法（它用于wikipedia）的实验性质的分词器。 还有用于其它语言的分词器，比如中文和俄语，还有ICUTokenizer会检测语言。另外NGramtokenizer会在后面讨论。可以在http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters中找到更多内容。 WordDelimiterFilter它也许不是一个正式的分词器，但是这个名为WordDeilimiterFilter的词元过滤器本质上是一个分词器。 123&lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/&gt; 上面并没有给出所有的选项，这个过滤器可以通过多种配置指定如切分和连接合成词，并有多种定义合成词的方法。这个过滤器通常与WhitespaceTokenizer配合，而不是StandardTokenizer。这个过滤器的配置中1是设置，0是重置。 WordDelimiterFilter先通过配置选项中的定义切分词元： 词间的分隔符切分：Agile-Me切为Agile，Me 字母和数据间的切分：SD500切为SD，500（如果设置splitOnNumerics） 忽略任何分隔符：hello,Agile-Me切为hello, Agile,Me 移除所有格’s：David’s切为Divid（如果设置stemEnglishPocessive） 在小写到大小时切分：Agile-Me切为agile,me（如果设置splitOnCaseChange） 此时，如果下面的选项没有设置，上面这些切分后的词都要被过滤掉。因为默认下面的选项设置为false，你一般至少要设置下面其中一项。 如果设置generateWordParts或是generateNumberParts，那么全是字母或是全是数字的词元就会不被过滤。他们还会受到连接选项的进一步影响。 连接多个全字母的词元，设置catenateWords（比如wi-fi连接为wifi）。如果generateWordParts设置了，这个例子还是会产生wi和fi，反过来不成立。catenateNumbers工作方式也是相似的。catenateAll会考虑连接所有的词到一起。 要保留原始的词，设置preserveOriginal。 下面是一个对上面选项的解释的例子： 1WiFi-802.11b 切为 Wi,Fi,WiFi,802,11,80211,b,WiFi80211b, WiFi-802.11b Stemming词干化是去除词尾变化或是有时将派生词变回它们的词干——基本形的过程。比如，一种词干化算法可能会将Riding和Rides转化为Ride。词干化有助于提高结果召回率，但是会对准确率造成负影响。如果你是处理普通文本，你用词干化会提高你的搜索质量。但是如果你要处理的文本都是名词，比如在MusicBrainz中的艺术家名字，那么深度的词干化可能会影响结果。如果你想提高搜索的准确率，并且不降低完整率，那么你可以考虑将数据索引到两个域，其中一个进行词干化，另一个不进行词干化，在搜索时查找这两个域。 大多词干器产生的词干化的词元都不再是一个拼写合法的单词，比如Bunnies会转化为Bunni，而不是Bunny，Quote转化为Quot，你可以在Solr的文本分析页面看到这些结果。如果在索引和查找时都进行词干化，那么是不会影响搜索的。但是一个域词干化之后，就无法进行拼写检查，通配符匹配，或是输入提示，因为这些特性要直接用索引中的词。 下面是一些适用于英文的词干器： SnowballPorterFilterFactory：这个词干器允许选择多种词干器算法，这些词干器算法是由一个名为Snowball的程序产生的。你可以在language属性中指定你要选择的词干器。指定为English会使用Porter2算法，它比原生的Porter的算法有一点点改进。指定为Lovins会使用Lovins算法，它比起Porter有一些改进，但是运行速度太慢。 PorterStemFIlterFactory：它是原生的英语Porter算法，它比SnowBall的速度快一倍。 KStemFilterFactory：这个英语词干器没有Porter算法激进。也就是在很多Porter算法认为应该词干化的时候，KSterm会选择不进行词干化。我建议使用它为默认的英语词干器。 EnglishMinimalStemFilterFactory：它是一个简单的词干器，只处理典型的复数形式。不同于多数的词干器，它词干化的词元是拼写合法的单词，它们是单数形式的。它的好处是使用这个词干器的域可以进行普通的搜索，还可以进行搜索提示。 Correcting and augmenting stemming上面提到的词干器都是使用算法进行词干化，而不是通过词库进行词干化。语言中有许多的拼写规则，所以算法型的词干器是很难做到完美的，有时在不应该进行词干化的时候，也进行了词干化。 如果你发现了一些不应该进行词干化的词，你可以先使用KeywordMarkerFilter词干器，并在它的protected属性中指定不需要词干化的词元文件，文件中一行一个词元。还有ignoreCase布尔选项。一些词干器有或以前有protected属性有相似的功能，但这种老的方式不再建议使用。 如果你需要指定一些特定的单词如何被词干化，就先使用StemmerOverrideFilter。它的dictionary属性可以指定一个在conf目录下的UTF-8编码的文件，文件中每行两个词元，用tab分隔，前面的是输入词元，后面的是词干化后的词元。它也有ignoreCase布尔选项。这个过滤器会跳过KeywordMarkerFilter标记过的词元，并且它会标记它替换过的词元，以使后面的词干器不再处理它们。 下面是三个词干器链在分析器中配置的示例： 12345&lt;filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt" /&gt;&lt;filter class="solr.StemmerOverrideFilterFactory" dictionary="stemdict.txt" /&gt;&lt;filter class="solr.PorterStemFilterFactory" /&gt; Synonyms进行同义词处理的目的是很好理解的，在搜索时搜索所用的关键词可能本身并不匹配文档中的任何一个词，但文档中有这个搜索关键词的同义词，但一般来讲你还是想匹配这个文档的。当然，同义词并一定不是按字典意义上同义词，它们可以是你应该中特定领域中的同义词。 这下一个同义词的分析器配置： 1&lt;filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/&gt; synonyms的属性值是在conf目录下的一个文件。设置ignoreCase为true在查找同义词时忽略大小写。 在我们讨论expand选项前，我们考虑一个例子。同义词文件是一行行的。下面是一个显式映射的例子，映射用=&gt;符号表示： 1i-pod, i pod =&gt;ipod 这表示如果在输入词元流中如果发现i-pod（一个词元）或是i pod（两个词元），都会替换为ipod。替换的同义词也可以是多个词元。逗号是分隔多个同义词之间的分隔符，同义词的词元间用空格分隔。如果你要实现自定义的不用空格分隔的格式，有一个tokenizerFactory属性，但它极少被使用。 你也可能看到配置文件里是这样的格式： 1ipod, i-pod, i pod 配置文件里没有=&gt;符号，它的意义由expand参数来决定，如果expand为true，它会被解释为下面的显式映射： 1ipod, i-pod, i pod =&gt;ipod, i-pod, i pod 如果expand设置为false，它就变为下面的显式映射，第一个同义词为替换同义词： 1ipod, i-pod, i pod =&gt;ipod 在多行中指定多个词替换为共一同义词是允许的。如果一个源同义词已经被规则替换了，另一个规则替换这个替换后词，则这两个规则可以合并。 Index-time versus query-time, and to expand or not如果你要进行同义词扩展，你可以在索引时或是查询时进行同义处理。但不要在索引和查询时都处理，这样处理会得到正确的结果，但是会减慢处理速度。我建议在索引时进行扩展，因为在查询时进行会有下面的问题： 一个源同义词包含多个词元（比如：i pod）不会在查询时被查询时被识别，因为查询解析器会在分析器处理之前就对空格进行切分。 如果被匹配的一个同义词在所有文档中很少出现，那么Lucene打分算法中的IDF值会很高，这会使得得分不准确。 前缀，通配符查询不会进行文本分析，所以不会匹配同义词。 但是任何在索引时进行的分文本处理都是不灵活的。因为如果改变了同义词则需要完全重建索引才能看到效果。并且，如果在索引时进行扩展，索引会变大，如果你使用WordNet类似的同义词规则，可能索引大到你不能接受，所以你在同义词扩展规则上应该选择一个合理的度，但是我通常还是建议在索引时扩展。 你也许可以采用一种混合策略。比如，你有一个很大的索引，所以你不想对它经常重建，但是你需要使新的同义词迅速生效，所以你可以将新的同义词在查询时和索引时都使用。当全量索引重建完成后，你可以清空查询同义词文件。也许你喜欢查询时进行同义词处理，但你无法处理个别同义词有空格的情况，你可以在索引时处理这些个别的同义词。 Stop WordsStopFilterFactory是一个简单的过滤器，它是过滤掉在配置中指定的文件中的停词（stop words），这个文件在conf目录下，可以指定忽略大小写。 1&lt;filter class="solr.StopFilterFactory" words="stopwords.txt" ignoreCase="true"/&gt; 如果文档中有大量无意义的词，比如“the”，“a”，它们会使索引变大，并在使用短语查询时降低查询速度。一个简单的方法是将这些词经常出现的域中过滤掉，在包含多于一句(sentence)的内容的域中可以考虑这种作法，但是如果把停词过滤后，就无法对停词进行查询了。所以如果你要使用，应该在索引和查询分析器链中都使用。这通常是可以接受的，但是在搜索“To be or not to be”这种句子时，就会有问题。对停词理想的做法是不要去过滤它们，以后介绍CommonGramsFilterFactory来解决这个问题。 Solr自带了一个不错的英语停词集合。如果你在索引非英语的文本，你要用自己指定停词。要确定你索引中有哪些词经常出现，可以从Solr管理界面点击进入SCHEMA BROWSER。你的字段列表会在左边显示，如果这个列表没有立即出现，请耐心点，因为Solr要分析你索引里的数据，所以对于较大的索引，会有一定时间的延时。请选择一个你知道包含有大量文本的域，你可以看到这个域的大量统计，包括出现频率最高的10个词。 Phonetic sound-like analysis语音转换（phonetic translation）可以让搜索进行语音相似匹配。语音转化的过滤器在索引和查询时都将单词编码为phoneme。有五种语音编码算法：Caverphone，DoubleMetaphone，Metaphone，RefinedSoundex和Soundex。有趣的是，DoubleMetaphone似乎是最好的选择，即使是用在非英语文本上。但也许你想通过实验来选择算法。RefinedSoundex声称是拼写检查应用中最适合的算法。然而，Solr当前无法在它的拼写检查组件中使用语音分析。 下面是在schema.xml里推荐使用的语音分析配置。 123456789101112131415&lt;!-- for phonetic (sounds-like) indexing --&gt;&lt;fieldType name="phonetic" class="solr.TextField" positionIncrementGap="100" stored="false" multiValued="true"&gt; &lt;analyzer&gt; &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt; &lt;filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="1" catenateNumbers="0" catenateAll="0"/&gt; &lt;filter class="solr.DoubleMetaphoneFilterFactory" inject="false" maxCodeLength="8"/&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 注意，语音编码内部忽略大小写。 在MusicBrainz Schema中，有一个名为a_phonetic使用这个域类型，它的域值是通过copyField拷贝的Artist名字。第四章你会学习到dismax查询解析器可以让你对不同的域赋不同的boost，同时查找这几个域。你可以不仅仅搜索a_name域，你还可以用一个较低的boost来搜索a_phonenic域，这样就可以进行兼顾语音搜索了。 用Solr的分析管理页面，你可以看到这它将Smashing Pumpkins编码为SMXNK|XMXNK PMPKNS（|表示两边的词元在同一位置）。编码后的内容看起来没什么意义，实际它是为比较相似语音的效率而设计。 上面配置示例中使用的DoubleMetaphoneFilterFactory分析过滤器，它有两个选项： Inject：默认设置为true，为true会使原始的单词直接通过过滤器。这会影响其它的过滤器选项，查询，还可能影响打分。所以最好设置为false，并用另一个域来进行语音索引。 maxCodeLength：最大的语音编码长度。它通常设置为4。更长的编码会被截断。只有DoubleMetaphone支持这个选项。 如果要使用其它四个语音编码算法，你必须用这个过滤器： 1&lt;filter class="solr.PhoneticFilterFactory" encoder="RefinedSoundex" inject="false"/&gt; 其中encoder属性值是第一段中的几个算法之一。 Substring indexing and wildcards通常，文本索引技术用来查找整个单词，但是有时会查找一个索引单词的子串，或是某些部分。Solr支持通配符查询（比如mus*ainz），但是支持它需要在索引时过行一定的处理。 要理解Lucene在索引时内部是如何支持通配符查询是很有用的。Lucene内部会在已经排序的词中先查询非通配符前缀（上例中的mus）。注意前缀的长度与整个查询的时间为指数关系，前缀越短，查询时间越长。事实上Solr配置Lucene中不支持以通配符开头的查询，就是因为效率的原因。另外，词干器，语音过滤器，和其它一些文本分析组件会影响这种查找。比如，如果running被词干化为run，而runni*无法匹配。 ReversedWildcardFilterSolr不支持通配符开头的查询，除非你对文本进行反向索引加上正向加载，这样做可以提高前缀很短的通配符查询的效率。 下面的示例应该放到索引文本分析链的最后： 1&lt;filter class="solr.ReversedWildcardFilterFactory" /&gt; 你可以在JavaDocs中了解一些提高效率的选项，但默认的就很不错：http://lucene.apache.org/solr/api/org/apache/solr/analysis/ReversedWildcardFilterFactory.html Solr不支持查询中同时有配置符在开头和结尾，当然这是出于性能的考虑。 N-gramsN-gram分析会根据配置中指定的子中最小最大长度，将一个词的最小到最大的子串全部得到，比如Tonight这个单词，如果NGramFilterFactory配置中指定了minGramSize为2，maxGramSize为5，那么会产生下面的索引词：(2-grams)：To, on , ni, ig, gh, ht，(3-grams)：ton, oni, nig, ight, ght, (4-grams)：toni, onig, nigh, ight, (5-grams)：tonig，onigh, night。注意Tonight完整的词不会产生，因为词的长度不能超过maxGramSize。N-Gram可以用作一个词元过滤器，也可以用作为分词器NGramTokenizerFactory，它会产生跨单词的n-Gram。 下是是使用n-grams匹配子串的推荐配置： 12345678910111213141516171819202122232425&lt;fieldType name="nGram" class="solr.TextField" positionIncrementGap="100" stored="false" multiValued="true"&gt; &lt;analyzer type="index"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;!-- potentially word delimiter, synonym filter, stop words, NOT stemming --&gt; &lt;filter class="solr.LowerCaseFilterFactory"/&gt; &lt;filter class="solr.NGramFilterFactory" minGramSize="2" maxGramSize="15"/&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;!-- potentially word delimiter, synonym filter, stop words, NOT stemming --&gt; &lt;filter class="solr.LowerCaseFilterFactory"/&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 注意n-Gram只在索引时进行，gram的大小配置是根据你想进行匹配子串的长度而决定 的（示例中是最小是2，最长是15）。 N_gram分析的结果可以放到另一个用于匹配子串的域中。用dismaxquery解析器支持搜索多个域，在搜索匹配这个子串的域可以设置较小的boost。 另一个变形的是EdgeNGramTokenizerFactory和EdgeNGramFilterFactory，它会忽略输入文本开头或结尾的n-Gram。对过滤器来说，输入是一个词，对分词器来说，它是整个字符流。除了minGramSize和maxGramSize之后，它还有一个side参数，可选值为front和back。如果只需要前缀匹配或是后缀匹配，那边EdgeNGram分析是你所需要的了。 N-gram costsn-Gram的代价很高，前面的例子中Tonight有15个子串词，而普通的文本分析的结果一般只有一个词。这种转换会产生很多词，也就需要更长的时间去索引。以MusicBrainz Schema为例，a_name域以普通方式索引并stored，a_ngram域对a_name中的值进行n-Gram分析，子串的长度为2-15。它不是一个stored域，因为Artist的名字已经保存在a_name中了。 a_name a_name + a_ngram Increase Indexing Time 46 seconds 479 seconds &gt; 10x Disk Size 11.7 MB 59.7 MB &gt; 5x Distinct Terms 203,431 1,288,720 &gt; 6x 上表给出了只索引a_name和索引a_name和a_ngram的统计信息。注意索引时间增加了10倍，而索引大小增加了5倍。注意，这才只是一个域。 注意如果变大minGramSize的大小，nGram的代价会小很多。Edge nGraming也代价也会小，因为它只关心开头或结尾的nGram。基于nGram的分词器无疑会比基于nGram的过滤器代码要高，因为分词器将产生带空格的词，然而，这种方式可以支持跨词的通配符。 Sorting Text通常，搜索结果是由神奇的score伪字段进行排序的，但是有时候也会根据某个字段的值进行排序。除了对结果进行排序，它还有许多的作用，进行区间查询和对Facet结果进行排序。 MusicBrainz提供了对Artist和Lable名称进行排序的功能。排序的版本会将原来的名字中的某些词，比如“The”移到最后，用逗号分隔。我们将排序的名字域设置为indexed，但不是stored，因为我们要对它进行排序，但不进行展示，这与MusicBrainz所实现的有所不同。记住indexed和stored默认设置为true。因为有些文本分析组件会限制text域的排序功能，所以在你的Schema中要用于排序的文本域应该拷贝到另一个域中。copyField功能会很轻松地完成这个任务。String类型不进行文本分析，所以它对我们的MusicBrainz情况是非常适合的。这样我们就支持了对Artist排序，而没有派生任何内容。 Miscellaneous token filtersSolr还包括许多其它的过滤器： ClassicFilterFactory：它与ClassicTokenizer配置，它会移除缩写词中的点号和末尾的’s：”I.B.M. cat’s” =&gt; “IBM”, “cat” EnglishProcessiveFilterFactory：移除’s。 TrimFilterFactory：移除开头和结尾的空格，这对于脏数据域进行排序很有用。 LowerCaseFilterFactory：小写化所有的文本。如果你要用WordDelimeterFilterFactory中的大小写转换切分功能，你就不要将这个过滤器放前面。 KeepWordFilterFactory：只保留指定配置文件中的词： 如果你想限制一个域的词汇表，你可以使用这个过滤器。 LengthFilterFactory：过滤器会过滤掉配置长度之间的词： LimitTokenCountFilterFactory：限制域中最多有多少个词元，数量由maxTokenCount属性指定。Solr的solrconfig.xml中还有设置，它对所有域生效，可以将它注释掉，不限制域中的词元个数。即使没有强制限制，你还要受Java内存分配的限制，如果超过内存分配限制，就会抛出错误。 RemoveDuplicatestTokenFilterFactory：保存重复的词不出现在同一位置。当使用同义词时这是可能发生的。如果还要进行其它的分本分析 ，你应该把这个过滤器放到最后。 ASCIIFoldingFilterFactory：参见前面的“Character filter”一节中的MappingCharFilterFactory。 CapitalizationFilterFactory：根据你指定的规则大写每个单词。你可以在http://lucene.apache.org/solr/api/org/apache/solr/analysis/CapitalizationFilterFactory.html中了解更多内容。 PatternReplaceFilterFactory：使用正则表达式查找替换。比如： 这个例子是处理e-mail地址域，只取得地址中的域名。Replacement是正则表达式中的组，但它也可以是一个字符串。如果replace属性设置为first，表示只替换第一个匹配内容。如果replace设置为all，这也是默认选项，则替换全部。 实现你自己的过滤器：如果现有的过滤器无法满足你的需求。你可以打开Solr的代码看一下里面是如何实现的。在你深入之前，你看PatternReplaceFilterFactory的实现是如此简单。作为一个初学者，可以看一下在本书提供的补充资料中schema.xml中的rType域类型。 还有其它各式各样的Solr过滤器，你可以在http://lucene.apache.org/solr/api/org/apache/solr/analysis/TokenFilterFactory.html 中了解所有的过滤器。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solr学习:SolrJ-操作Solr的API]]></title>
      <url>%2F2017%2F03%2F10%2FSolr%E5%AD%A6%E4%B9%A0-SolrJ-%E6%93%8D%E4%BD%9CSolr%E7%9A%84API%2F</url>
      <content type="text"><![CDATA[SolrJ的概念 SolrJ是一个Java连接Solr来对索引进行增删改查的一个jar包。它的底层是通过使用HttpClient来实现对Solr的操作的。不过SolrJ对HttpClient进行了封装，提供了大量接口，大大简化了开发者的工作。毕竟，谁会愿意在有API的情况下，还通过HttpClient去进行Solr操作呢。具体了解更多，可以参看SolrJ的wiki页：链接直达 SolrJ所需要的jar包 创建完maven项目，在pom文件中加入以下依赖。 1234567891011121314151617181920&lt;!-- solrj --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.solr&lt;/groupId&gt; &lt;artifactId&gt;solr-solrj&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- junit单元测试 --&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt;&lt;/dependency&gt;&lt;!-- lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.14&lt;/version&gt;&lt;/dependency&gt; 创建Demo代码 创建索引的POJO类创建和Solr索引一样的POJO对象，通过@Field字段注解，其value值为Solr内对应的field name。因为此处我用了lombok，所以类属性的get和set方法通过注解完成了。 123456789101112131415161718192021222324252627public class Goods &#123; @Field("_root_") @Getter @Setter private String root; @Field("_version_") @Getter @Setter private Long version; @Field("id") @Getter @Setter private String id; @Field("name") @Getter @Setter private String name; @Field("sku") @Getter @Setter private String sku;&#125; 创建Client接口并实现SolrJ的Client接口： 12345678910111213141516171819202122232425262728public interface ISolrJClientService &#123; /** * 连接至Solr */ void linkToSolr(); /** * 添加Goods信息 * @param goods */ void insertGoods(Goods goods) throws IOException, SolrServerException; /** * 根据Goods名称查询GoodsList * @param name 名称 * @param pageNo 页码 * @param rows 显示记录条数 * @return GoodsList */ List&lt;Goods&gt; selectGoodsListByName(String name, int pageNo, int rows) throws IOException, SolrServerException; /** * 根据ID删除Goods * @param goodsId ID */ void deleteGoodsById(String goodsId) throws IOException, SolrServerException;&#125; SolrJ的Client接口实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class SolrJClientService implements ISolrJClientService &#123; private HttpSolrClient httpSolrClient; /** * 连接至Solr */ @Override public void linkToSolr() &#123; // ip:port/solr应用名/coreName String url = "localhost:8080/solr/fuukangunCore"; HttpSolrClient httpSolrClientTemp = new HttpSolrClient(url); // 设置响应解析器 httpSolrClientTemp.setParser(new XMLResponseParser()); // 建立连接的最长时间 httpSolrClientTemp.setConnectionTimeout(500); httpSolrClient = httpSolrClientTemp; &#125; /** * 添加Goods信息 * * @param goods */ @Override public void insertGoods(Goods goods) throws IOException, SolrServerException&#123; httpSolrClient.addBean(goods); httpSolrClient.commit(); &#125; /** * 根据Goods名称查询GoodsList * * @param name 名称 * @param pageNo 页码 * @param rows 显示记录条数 * @return GoodsList */ @Override public List&lt;Goods&gt; selectGoodsListByName(String name, int pageNo, int rows) throws IOException, SolrServerException &#123; // Solr查询结构体 SolrQuery solrQuery = new SolrQuery(); // 设置分页 solrQuery.setStart((Math.max(pageNo, 1) - 1) * rows); solrQuery.setRows(rows); // 生成查询语句，即在控制台使用的q= String query = "name:" + name; solrQuery.setQuery(query); // 进行查询 QueryResponse queryResponse = httpSolrClient.query(solrQuery); List&lt;Goods&gt; goodsList = queryResponse.getBeans(Goods.class); return goodsList; &#125; /** * 根据ID删除Goods * * @param goodsId ID */ @Override public void deleteGoodsById(String goodsId) throws IOException, SolrServerException &#123; httpSolrClient.deleteById(goodsId); httpSolrClient.commit(); &#125;&#125; 创建JUnit测试类操作Solr的前提是连接到Solr，所以连接Test方法专门用了@Before注解。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class SolrClientServiceTest &#123; // 没有用Spring框架，需要实例化 private ISolrJClientService solrjClientService = new SolrJClientService(); /** * 运行前连接Solr */ @Before public void linkToSolrTest() &#123; solrjClientService.linkToSolr(); &#125; /** * 添加原料测试 */ @Test public void insertGoodsTest() &#123; Goods goods = new Goods(); String uuid = UUID.randomUUID().toString().replaceAll("-", ""); goods.setId(uuid); goods.setName("牛肉通过Solrj添加"); try &#123; solrjClientService.insertGoods(goods); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Test public void selectGoodsListTest() &#123; int pageNo = 1; int rows = 20; String name = "牛肉"; try &#123; List&lt;Goods&gt; goodsList = solrjClientService.selectGoodsListByName(name, pageNo, rows); if (null != goodsList &amp;&amp; goodsList.size() &gt; 0) &#123; for (Goods goods : goodsList) &#123; System.out.println(goods.getName()); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Test public void deleteGoodsTest() &#123; String id = "9339adc6e4504ba79c9dc505d72e2674"; try &#123; solrjClientService.deleteGoodsById(id); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 结束语 一定有人发现了说好的增删改查，为什么没有update？原因很简单，忘记写了。有空再补(大概会忘记的)，或者大家自行百度吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solr学习:managed-schema详解]]></title>
      <url>%2F2017%2F03%2F09%2FSolr%E5%AD%A6%E4%B9%A0-managed-schema%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[前言 本文很大程度借鉴抄袭转载自：一剑侵心的博客:关于Solr学习的那部分 在公司自己搭建Solr玩的时候，因为用的JDK是1.7版本的，所以安装了Solr5.3.1(大概是这个版本吧，记不清了，也可能是4.x的)。当时的配置文件叫schema.xml，搞得我一开始没找到配置文件，还以为在官网下了一个假Solr。结果Solr不知道从5.x版本开始，虽然配置文件还是xml格式，但是改名叫managed-schema，还是不带后缀的那种，简直了。 其实managed-schema的格式简单来看就是下面这个样子的： 12345678&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;schema name="example" version="1.6"&gt; &lt;field/&gt; &lt;dynamicField/&gt; &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; &lt;copyField/&gt; &lt;fieldType/&gt;&lt;/schema&gt; field元素 特殊之处field用来配置document中的字段，类似于数据库中的column。但是这里有三个特殊一点的字段，分别是：_version_, _root_ 和 id。 _version_如果去除此字段，必须同时去掉solrconfig.xml中的update log。_version_ and update log 在SolrCloud中是必须的。作用类似于hibernate中的version字段，用于乐观锁。 _root_如果document中内嵌document，此需要此字段。用于内嵌的document指向其父document。 id除非你有很充足的理由，否则不要去除”id”字段。不要改变type属性， 不要对对应的字段进行索引时分析。 field元素的属性 属性名称 默认值 属性说明 name 必填 type 必填，值必须定义在中 indexed true 是否进行索引。 true的时候进行索引。 stored true 是否存储。如果此字段的值需要显示在搜索结果中，则需要进行存储。 docValues fasle 是否需要存储docValues。true为设置。docValues用于提升sorting, faceting, grouping, function queries等性能，现在仅支持StrField, UUIDFiel和所有的Trie*Field，此值为true的字段要求此字段 multiValued=false，并且 (required=true或设置了default的值)。 multiValued fasle 是否有多个值。 omitNorms 如果你的大部分的document的长度大小都差不多，则设置成true。如果此字段在索引时需要boost，则设置为false。 termVectors fasle 设置为true，使More Like This特性生效，会极大的增加索引文件的大小。 termPositions fasle 通常用于提高高亮搜索结果这一功能的性能。设置为true，会增加索引文件的大小。 termOffsets fasle 通常用于提高高亮搜索结果这一功能的性能。设置为true，会增加索引文件的大小。 termPayloads fasle 通常用于提高高亮搜索结果这一功能的性能。设置为true，会增加索引文件的大小。 required fasle 是否为必填值。 如果设置为true，则索引时，如果此字段值为null，则会报错。如果是字符串，亲测使用””也会报错。 default 这个field的默认值。 dynamicField元素 动态字段 属性配置上与常规字段没啥区别，最大的区别是name的属性上可以进行通配，比如说name=”*_i”，那么只要是后面带i的字段都是符合的。这样就不怕一些字段无法匹配无法写入。 uniqueKey元素 类似于数据库中的主键。配置文件默认的是id。我们在数据库设计时，虽然不强制每个表有主键，但是一般情况下还是会设置一个主键的。同样这个元素也不是必须的，但是强烈建议设置此值。 copyField 假设有这样一个需求：根据书名或者作者名查询图书馆的图书。如果在数据库中，我们可以写成select * from book where author like `%东野圭吾%` or book_name like `%东野圭吾%`。但是在Solr中没有这样的语法。所以，我们需要把author字段和book_name字段拷贝到一个新字段中，通过这个字段来实现上面的需求。写法如下： 123&lt;field name="text" type="string" index="true" stored="true" multiValues="true" /&gt;&lt;copyField source="author" dest="text" /&gt;&lt;copyField source="book_name" dest="text" /&gt; 这样我们就只需要搜索text里的内容就可以了。 注意：如果dest由多个source构成，就需要将其指定为multiValued。 fieldType fieldType主要定义了一些字段类型，其name属性值用于前面中的type属性的值。e.g. 其中class属性中”solr”是org.apache.solr.schema这个包名的缩写。 fieldType的属性： name 由字母、数字和下划线组成。不能以数字开头。此值用于前面中的type属性的值。 class 此值表明索引并存储此fieldType的数据的类型（e.g 字符类型，数字类型，日期类型…）。如果此类不是solr提供的（自定义的或第三方的类），则不能用”solr.”，需要写类的全路径名。 positionIncrementGap 值为整数，用于multiValued=”true”的字段，指定多个值之间的距离，以防出现假的短语匹配。 比如描述书本作者的字段是有多个值的，假设有两个作者：John Smith 和 Mike Jackson，我们搜索”Smith Mike”这个作者，如果positionIncrementGap值设成0，则此记录就会被认为是匹配搜索条件的，实际上是不匹配的。对于这种情况，我们应该把此值设置成一个较大的值，比如100。 autoGeneratePhaseQueries 值为布尔类型。默认值为false。设为true时，会自动生成短语查询。 举个粟子：索引中的文本内容为：春花秋月何时了…一江春水向东流。 我们在搜索的输入框里输入”春花”（注意不输入两个双引号），如果autoGeneratePhaseQueries 为true，我们加上highlight的话，返回的匹配结果为： 春花春花秋月何时了…一江春水向东流。 如果值为false，则返回结果为春花春花秋月何时了…一江春水向东流。 如果值为false，我们还是想要进行短语查询，可在输入框里输入”春花”(注意需要加上两个双引号)。 docValuesFormat 自定义docValues的格式。设置此值的话，必须在solrconfig.xml里配置schema-aware codec。如： 1&lt;codecFactory class="solr.SchemaCodecFactory" /&gt; 在网上搜了一下，只看到有两个值 Memory 和 Disk。猜想这个属性的作用应该是定义docValues值是存在硬盘上还是存在内存中吧。 postingsFormat 自定义PostingsFormat。设置此值的话，必须在solrconfig.xml里配置schema-aware codec。不太清楚具体有什么用。 注：尽量不要使用docValuesFormat和postingFormat。Solr的guideline上有一段话，翻译如下： 仅当使用默认的codec 时，Lucene索引才支持向后兼容。因此，如果使用了这两个属性，那么将来想要升级到更高版本的Solr 时，需要你切换回默认的codec，然后优化现有的索引或者重新建立整个索引。 以下的属性也同时存在于里，如果里的值会覆盖里的值。 indexed 布尔值。true表示进行索引。 stored 布尔值。true表示进行存储。 docValues 布尔值。true表示field的值将会被存储于面向列的数据结构中。 sortMissingFirst 布尔值。true表示排序的时候，此field值为空的记录排在此field值不为空的记录的前面。 sortMissingLast 布尔值 。意思和sortMissingFirst相反。 multiValues 布尔值。 omitNorms 布尔值。 omitTermFreqAndPositions 布尔值。忽略term frequency, positions 和 payloads。所有非文本类型字段，此默认值是true。 omitPositions 布尔值。布尔值。忽略positions。 termVectors， termPositions， termOffsets 和 termPayloads 布尔值。 required 布尔值。 useDocValuesAsStored 布尔值。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solr学习:为Solr添加数据]]></title>
      <url>%2F2017%2F03%2F04%2FSolr%E5%AD%A6%E4%B9%A0-%E4%B8%BASolr%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE%2F</url>
      <content type="text"><![CDATA[schema配置简介 为什么此处只做一个简介，而不是放一篇详解上来？因为本文主要讲解如何为Solr添加数据，对于schema配置的详解，不是重点。想要了解清楚的各位，可以自行查询，网上很多关于schema的详解。或者也可以直接参看相关的官方wiki：链接直达 把Solr的一个Core当作数据库的一张表，那么其schema就相当于这个表的表结构设计。（本人刚刚接触Solr，如果表述有误，欢迎指正）也就是说，在添加数据之前，我们得先设计好表结构。 那么，schema的配置文件在哪呢？其实就在solr_home/solr/fuukangun（此处是你的Core name）/conf下找到managed-schema（之前的版本是schema.xml文件）。 打开managed-schema文件，我们可以看到里面默认配置了很多filed，也就是说默认了很多字段。你可以将里面的filed全删除，配上你想要的字段。但是值得注意的是，类似_version_和_root_这种带下划线的字段，是系统保留字段，可以不删除，但建议你的数据库中，表字段不要类似命名。 由于已经有默认配置了，那么我在这里就直接用原本自带的部分字段好了。这里选用id，sku，name三个字段，将其他的field注释或删除。看这几个field名称，应该是用来存储商品信息的。 123&lt;field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" /&gt;&lt;field name="sku" type="string" indexed="true" stored="true" omitNorms="true"/&gt;&lt;field name="name" type="string" indexed="true" stored="true"/&gt; 在Solr控制台添加数据 在Solr控制台选择核心以后，点击下方的Documents按钮。Solr上一条数据可以看成是一个Document。 Document Type可以选择很多，我在这里选择了JSON。然后在下面填入JSON格式的数据，点击Submit Document。右下角显示Status: success就表示数据添加成功了。我们也可以在这里添加一个数组的对象，不一定要一个个添加。 进行数据查询 选择左边菜单栏里面的Query，点击最下面的Execute Query按钮就会在右边出现查询结果了。上面的q输入框内，就是输入查询条件的，*:*就是检索所有条件的。比如我们这个索引只有三个字段：id，sku，name。我们根据name模糊查询的话，可以写成name:雀巢*。*的作用，类似于MySQL中的%。当然，我们也可以精确查询。另外一些设置这里不在简述。 1. 问题出现 但是，在点击了Execute Query按钮之后，界面上并没有出现任何数据。于是通过调试，我在控制台发现了服务器400的错误。个人怀疑是Tomcat上解析URL出错了，我把URL复制下来，粘贴到浏览器的地址栏，把%2F修改为/。结果成功获得了刚刚添加过的数据。 2. 解决方式 找到Tomcat/conf目录下的catalina.properties文件，在最下面添加如下代码：org.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true重启Tomcat，就能使Tomcat正确解析出URL了。 3. 结果查看 然后我们再进一次Solr控制台，就能看到搜索结果了。 从MySQL导入数据到Solr 解决了上面的问题，我们继续来尝试一下如何从数据库导入数据到Solr。 首先在数据库建立相同格式的表，并在表里加几条数据。 要想Solr连接数据库，需要两个jar包： mysql-connector-java-5.1.41-bin.jar（这个不需要多说，没有的可以自行下载） solr-5.5.0/dist目录下，solr-dataimporthandler-5.5.0.jar 将这两个包，拷贝到Tomcat\webapps\solr\WEB-INF\lib目录下。 配置solr_home/solr/yourCore/conf目录下的solrconf.xml文件。在前面上加上一个dataimport的处理的Handler 12345&lt;requestHandler name="/dataimport" class="org.apache.solr.handler.dataimport.DataImportHandler"&gt; &lt;lst name="defaults"&gt; &lt;str name="config"&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 在同目录下，创建data-config.xml文件 dataSource是数据库数据源。 Entity就是一张表对应的实体，pk是主键，query是查询语句。 Field对应一个字段，column是数据库里的column名，后面的name属性对应着Solr的Filed的名字。 123456789101112&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;dataConfig&gt; &lt;dataSource type="JdbcDataSource" driver="com.mysql.jdbc.Driver" url="jdbc:mysql://127.0.0.1:3306/solr_data" user="root" password="root" batchSize="-1" /&gt; &lt;document name="goods"&gt; &lt;entity name="goods" pk="id" query="select * from goods"&gt; &lt;field column="id" name="id"/&gt; &lt;field column="sku" name="sku"/&gt; &lt;field column="name" name="name"/&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 重启Tomcat，在Solr控制台，选择Dataimport，点击Execute从MySQL导入数据。可能需要稍微等几秒，点击Refresh Status刷新倒入状态，出现下图，表示数据导入成功。 点击Query查询数据 但是原本我们通过控制台添加的数据消失了，这是因为我们在上一步点击Execute导入数据时，勾选了clean的缘故。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solr学习:Core配置]]></title>
      <url>%2F2017%2F02%2F25%2FSolr%E5%AD%A6%E4%B9%A0-Core%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[前言 上回说到，如何搭建Solr5.5.0，进入Solr控制台。但是，可以看到，此时控制台左边Core选择栏，显示的是No cores available。也就是说，Solr从5.X开始就不提供默认的Core了，而需要我们自行配置。本文就讲述如何配置第一个Core。 配置年轻人的第一个Core 首先，进入你原来配置的solr_home/solr目录下，创建一个文件夹，命名成你的CoreName。比如我创建了fuukangunCore。 接着，将solr_home/configsets/sample_techproducts_configs下的conf文件夹拷贝到solr_home/solr/fuukangunCore文件夹里。 然后，启动Tomcat，进入Solr控制台，点击左边的No cores available，然后Add Core。在name和instanceDir中填入你的CoreName。比如笔者就要填入fuukangunCore。注：Solr Core的命名有一定规范，具体可自行百度 最后，原本的No cores available就变成了Core Selector，在此可以选择你的Core。你也可以创建更多的Core。现在，你的Core目录下，应该多了core.properties文件和data文件夹。data文件夹式用于存储索引文件的，而core.properties文件则存放了Core的一些配置信息。 但是虽然我们有了Core，依旧缺少核心的东西，就是数据。下一次，我将会介绍如何配置Core的schema和从MySQL导入数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solr学习:Solr5.5.0搭建]]></title>
      <url>%2F2017%2F02%2F24%2FSolr%E5%AD%A6%E4%B9%A0-Solr5-5-0%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[前言 最近在工作上，由于DB使用了Cobar进行分库，所以表关联无法得以实现，需要在Solr上来进行一些复杂的查询。而且即便不进行分库，DB操作也不建议大数据量的多表进行关联查询，尤其是那种4、5张大表进行join的操作。然后顺带学习和了解一下Solr。 需要注意的是Solr6以上版本需要JDK8，Solr5以上版本需要JDK7。 Solr搭建 下载Solr 5.5.0，并解压到当前目录下。 下载Tomcat，此处我用的是Tomcat8.5.11。 将solr-5.5.0/server/solr-webapp下的webapp文件夹拷贝到Tomcat/webapps下，并将拷贝过来的webapp重命名成solr。（此处建议先把Tomcat/webapps下内容全部清空） 把solr-5.5.0/server/lib/ext中的jar包复制到Tomcat/webapps/solr/WEB-INF/lib目录中。 在任意位置新建solr_home文件夹，我建立在/Users/fuukangun路径下。把solr-5.5.0/server下的solr文件夹，拷贝到刚刚创建的solr_home文件夹下。solr_home就是之后创建索引实例Core的需要的目录 把solr-5.5.0/server/resources下的log4j.properties文件拷贝到Tomcat/webapps/solr/WEB-INF/classes目录下（classes不存在需要手动创建） 打开Tomcat/webapps/solr/WEB-INF下的web.xml文件。找到下面的配置内容，配置solr_home，一开始应该是被注释掉了。 12345&lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/put/your/solr/home/here&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt; &lt;/env-entry&gt; 将env-entry-value值替换为/Users/fuukangun/solr_home/solr，保存。 启动Tomcat，在浏览器输入http://localhost:8080/solr/index.html#/ 就能出现Solr控制台了。]]></content>
    </entry>

    
  
  
</search>
